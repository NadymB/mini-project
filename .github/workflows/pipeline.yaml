name: TopDev Jobs Data Pipeline

on:
  schedule:
    - cron: "0 */1 * * *"
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      # 1. Clone repo to VM
      - name: Checkout source
        uses: actions/checkout@v4

      # 2. Install Python to VM
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium

      # 4. Run crawler
      - name: Crawl TopDev jobs
        run: |
          python -m src.crawls.crawl_data_top_dev

      # 5. Run ETL pipeline
      - name: Run pipeline
        env:
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          python main.py
